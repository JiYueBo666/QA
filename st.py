import sys, os
from io import StringIO

sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
import streamlit as st
import requests
import time
from openai import OpenAI

from LLM.llm import largeModel, chatManager


# è®¾ç½®é¡µé¢æ ‡é¢˜
st.set_page_config(page_title="é—®ç­”ç³»ç»Ÿ", layout="centered")
st.title("æ··åˆæ£€ç´¢é—®ç­”ç³»ç»Ÿ")

# å®šä¹‰ API åœ°å€ï¼ˆæ ¹æ®ä½ çš„ FastAPI æœåŠ¡åœ°å€ä¿®æ”¹ï¼‰
API_URL = "http://localhost:8000/api/qa"

ai = largeModel()

# åˆå§‹åŒ– session_state
if "history_messager" not in st.session_state:
    st.session_state.history_messager = chatManager(
        "ä½ æ˜¯ä¸€ä¸ªåŒ»ç–—é—®ç­”åŠ©æ‰‹ï¼Œ"
        "è¯·æ ¹æ®ç”¨æˆ·çš„é—®é¢˜å’Œæ£€ç´¢ç»“æœåˆ°çš„ç›¸å…³è¯Šæ–­å†å²æä¾›å‡†ç¡®çš„ç­”æ¡ˆï¼Œç»™äºˆç”¨æˆ·å»ºè®®"
    )

# ä½¿ç”¨ session_state ä¸­çš„ history_messager
history_messager = st.session_state.history_messager

# åˆ›å»ºè¾“å…¥æ¡†
user_query = st.chat_input("è¯·è¾“å…¥ä½ çš„é—®é¢˜ï¼š")
if user_query:
    print(f"å½“å‰çš„ä¸Šä¸‹æ–‡:{history_messager.get_message()}")
    try:
        start = time.time()
        with st.status("æ­£åœ¨æ£€ç´¢é—®é¢˜...", expanded=True) as status:
            results = requests.post(API_URL, json={"query": user_query})
            end = time.time()
            status.update(
                label=f"æ£€ç´¢å®Œæˆï¼Œç”¨æ—¶:{end - start:.2f}s",
                state="complete",
                expanded=True,
            )

        if results.status_code == 200:
            results = results.json()

            # åˆ›å»ºä¸€ä¸ªå ä½ç¬¦ç”¨äºæ˜¾ç¤ºâ€œç”Ÿæˆä¸­â€å’Œâ€œç”Ÿæˆå®Œæˆâ€çš„æç¤º
            spinner_placeholder = st.empty()
            spinner_placeholder.info("ğŸ§  æ­£åœ¨ç”Ÿæˆå›å¤...")

            # è®°å½•ä¸Šä¸‹æ–‡å†å²ï¼Œè§£ææœç´¢ç»“æœ
            query = history_messager.parse_search_results(user_query, results)
            history_messager.add_message(role="user", content=query)
            current_messages = history_messager.get_message()
            print(f"æ¥å—ç”¨æˆ·è¾“å…¥åçš„ä¸Šä¸‹æ–‡:{current_messages}")
            # è·å–æµå¼å“åº”
            response_stream = ai.chat(messages=current_messages)
            full_response = []
            first_token_received = False

            # å®šä¹‰ä¸€ä¸ªç”Ÿæˆå™¨å‡½æ•°ä¾› st.write_stream ä½¿ç”¨
            def stream_output():
                first_token_received = False
                for chunk in response_stream:
                    content = chunk.choices[0].delta.content
                    if content:
                        if not first_token_received:
                            first_token_received = True
                            spinner_placeholder.success("âœ… ç”Ÿæˆå®Œæˆ")
                        full_response.append(content)
                        yield content

            st.write_stream(stream_output())

            # æ­¤æ—¶ full_response å·²ç»è¢«å¡«å……
            full_content = "".join(full_response)
            # å›å¤åŠ å…¥å†å²
            history_messager.add_message(role="assistant", content=full_content)

        else:
            st.error(f"è¯·æ±‚å¤±è´¥ï¼ŒçŠ¶æ€ç : {results.status_code}")
    except Exception as e:
        st.error(f"è¿æ¥é”™è¯¯: {e}")
